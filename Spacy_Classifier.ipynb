{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building NER model to identify the entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = [\n",
    "    (\"Who is Shaka Khan?\", {\"entities\": [(7, 17, \"H0\")]}),\n",
    "    (\"I like London and Berlin.\", {\"entities\": [(7, 13, \"H1\"), (18, 24, \"H0\")]}),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"sent_classifier_train.csv\", index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing Training Data for Spacy\n",
    "#table = str.maketrans('[]','')\n",
    "TRAIN_DATA=[]\n",
    "for index, row in data.iterrows():\n",
    "    #print(row['Spacy'])\n",
    "    entities_dict={}\n",
    "    \n",
    "    sentence=row['Sentence']\n",
    "    entities=row['Spacy'].split(\"],\")\n",
    "    entities_list=[]\n",
    "    if(len(row['Spacy'].strip())>2):\n",
    "        for entity_str in entities:\n",
    "            #entity.translate(table)\n",
    "            req_entity_str=entity_str.replace('[','').replace(']','')\n",
    "            req_entity=req_entity_str.split(',')\n",
    "            start_index=int(req_entity[0].strip())\n",
    "            #print(start_index)\n",
    "            end_index=int(req_entity[1].strip())\n",
    "            #print(end_index)\n",
    "            entity=req_entity[2].strip().replace(\"'\",\"\")\n",
    "            #print(entity)\n",
    "            entities_list.append((start_index,end_index,entity))\n",
    "    entities_dict['entities']=entities_list\n",
    "    TRAIN_DATA.append((row['Sentence'],entities_dict))\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Isaac David Abella (June 20, 1934 â€“ October 23, 2016) was a Professor of Physics at The University of Chicago. ',\n",
       " {'entities': [(0, 4, 'H0'),\n",
       "   (6, 10, 'H0'),\n",
       "   (12, 17, 'H0'),\n",
       "   (25, 28, 'H1'),\n",
       "   (29, 33, 'H1'),\n",
       "   (34, 35, 'H1'),\n",
       "   (36, 43, 'H1'),\n",
       "   (44, 47, 'H1'),\n",
       "   (48, 53, 'H1'),\n",
       "   (54, 57, 'H1'),\n",
       "   (60, 69, 'H1'),\n",
       "   (70, 72, 'H1'),\n",
       "   (73, 80, 'H1'),\n",
       "   (81, 83, 'H1'),\n",
       "   (84, 87, 'H1'),\n",
       "   (88, 98, 'H1'),\n",
       "   (99, 101, 'H1'),\n",
       "   (102, 110, 'H1')]})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DATA[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for Spacy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-59-34251adb9d5b>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-59-34251adb9d5b>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    )\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "@plac.annotations(\n",
    "    model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n",
    "    new_model_name=(\"New model name for model meta.\", \"option\", \"nm\", str),\n",
    "    output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n",
    "    n_iter=(\"Number of training iterations\", \"option\", \"n\", int),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model=None, new_model_name=\"Privacy_Redaction\", output_dir=\"Spacy_Model\", n_iter=300):\n",
    "    \"\"\"Set up the pipeline and entity recognizer, and train the new entity.\"\"\"\n",
    "    random.seed(0)\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "    # Add entity recognizer to model if it's not in the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner)\n",
    "    # otherwise, get it, so we can add labels to it\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "        \n",
    "    #Add H0 and H1 as entities\n",
    "    ner.add_label(\"H0\") \n",
    "    ner.add_label(\"H1\") \n",
    "    \n",
    "    if model is None:\n",
    "        optimizer = nlp.begin_training()\n",
    "    else:\n",
    "        optimizer = nlp.resume_training()\n",
    "        \n",
    "    move_names = list(ner.move_names)\n",
    "    \n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "    \n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        sizes = compounding(1.0, 4.0, 1.001)\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            batches = minibatch(TRAIN_DATA, size=sizes)\n",
    "            losses = {}\n",
    "            for batch in batches:\n",
    "                #print(batch)\n",
    "                texts, annotations = zip(*batch)\n",
    "                #print(zip(*batch))\n",
    "                #print(text)\n",
    "                #print(annotationsations)\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
    "            print(\"Losses\", losses)\n",
    "    \n",
    "    # test the trained model\n",
    "    test_text = \"Do you like horses?\"\n",
    "    doc = nlp(test_text)\n",
    "    print(\"Entities in '%s'\" % test_text)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.label_, ent.text)\n",
    "        \n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.meta[\"name\"] = new_model_name  # rename model\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        # Check the classes have loaded back consistently\n",
    "        assert nlp2.get_pipe(\"ner\").move_names == move_names\n",
    "        doc2 = nlp2(test_text)\n",
    "        for ent in doc2.ents:\n",
    "            print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n",
      "Losses {'ner': 6984.608969391234}\n",
      "Losses {'ner': 5149.077116403521}\n",
      "Losses {'ner': 4378.090314007212}\n",
      "Losses {'ner': 3734.79931748015}\n",
      "Losses {'ner': 3281.2561587755295}\n",
      "Losses {'ner': 3004.867279932058}\n",
      "Losses {'ner': 2696.5801871788663}\n",
      "Losses {'ner': 2399.5476219243337}\n",
      "Losses {'ner': 2269.5227938266244}\n",
      "Losses {'ner': 2151.233255962891}\n",
      "Losses {'ner': 1967.4936714395074}\n",
      "Losses {'ner': 1778.8185092237288}\n",
      "Losses {'ner': 1739.3674873866144}\n",
      "Losses {'ner': 1566.7262238851197}\n",
      "Losses {'ner': 1499.9920394995888}\n",
      "Losses {'ner': 1438.9841819992075}\n",
      "Losses {'ner': 1406.8210648244315}\n",
      "Losses {'ner': 1246.8125117346779}\n",
      "Losses {'ner': 1354.6964409755158}\n",
      "Losses {'ner': 1279.0518894672423}\n",
      "Losses {'ner': 1151.19263188842}\n",
      "Losses {'ner': 1082.1056158472381}\n",
      "Losses {'ner': 1126.5109987091773}\n",
      "Losses {'ner': 1027.8371648012994}\n",
      "Losses {'ner': 974.516844457925}\n",
      "Losses {'ner': 1083.5228720340042}\n",
      "Losses {'ner': 921.4054606219256}\n",
      "Losses {'ner': 913.197112164008}\n",
      "Losses {'ner': 830.1560100584119}\n",
      "Losses {'ner': 864.4408725349383}\n",
      "Losses {'ner': 862.9981591611792}\n",
      "Losses {'ner': 852.0886639829623}\n",
      "Losses {'ner': 819.8227460223981}\n",
      "Losses {'ner': 848.8401068931752}\n",
      "Losses {'ner': 753.9388225016512}\n",
      "Losses {'ner': 746.3800441403988}\n",
      "Losses {'ner': 730.695120434991}\n",
      "Losses {'ner': 704.5437804251577}\n",
      "Losses {'ner': 713.4977407962486}\n",
      "Losses {'ner': 671.3099945725936}\n",
      "Losses {'ner': 732.9383242410559}\n",
      "Losses {'ner': 695.9045326200759}\n",
      "Losses {'ner': 615.1447637656717}\n",
      "Losses {'ner': 672.5051665322737}\n",
      "Losses {'ner': 615.5391575278609}\n",
      "Losses {'ner': 622.8688331751382}\n",
      "Losses {'ner': 619.9364124971725}\n",
      "Losses {'ner': 605.0850686230208}\n",
      "Losses {'ner': 597.5332944308024}\n",
      "Losses {'ner': 556.2557157292746}\n",
      "Losses {'ner': 619.2625473806263}\n",
      "Losses {'ner': 553.8651147479811}\n",
      "Losses {'ner': 566.7108428880291}\n",
      "Losses {'ner': 499.2898770606351}\n",
      "Losses {'ner': 577.8438761182248}\n",
      "Losses {'ner': 489.1502368803732}\n",
      "Losses {'ner': 518.641645749797}\n",
      "Losses {'ner': 506.5398162347616}\n",
      "Losses {'ner': 518.6500319157974}\n",
      "Losses {'ner': 515.7176270258029}\n",
      "Losses {'ner': 506.33964989381866}\n",
      "Losses {'ner': 479.84748851221354}\n",
      "Losses {'ner': 520.9662266493855}\n",
      "Losses {'ner': 528.1009322717987}\n",
      "Losses {'ner': 463.62410550676873}\n",
      "Losses {'ner': 396.25697683452125}\n",
      "Losses {'ner': 430.4432000239474}\n",
      "Losses {'ner': 433.84859783024973}\n",
      "Losses {'ner': 499.54557929601094}\n",
      "Losses {'ner': 432.92492655607714}\n",
      "Losses {'ner': 376.78582945028194}\n",
      "Losses {'ner': 435.42392674854534}\n",
      "Losses {'ner': 354.1119739758393}\n",
      "Losses {'ner': 438.21683928689254}\n",
      "Losses {'ner': 395.21625524149624}\n",
      "Losses {'ner': 377.4363507127056}\n",
      "Losses {'ner': 352.4740327812021}\n",
      "Losses {'ner': 452.42518361221977}\n",
      "Losses {'ner': 372.04105218057015}\n",
      "Losses {'ner': 326.66187495860555}\n",
      "Losses {'ner': 419.34857001266874}\n",
      "Losses {'ner': 363.65566018843685}\n",
      "Losses {'ner': 372.4334442439468}\n",
      "Losses {'ner': 356.32464366612265}\n",
      "Losses {'ner': 306.7332257006389}\n",
      "Losses {'ner': 349.7515812837836}\n",
      "Losses {'ner': 363.66116430711116}\n",
      "Losses {'ner': 402.4020378986042}\n",
      "Losses {'ner': 365.1299689241504}\n",
      "Losses {'ner': 311.0519140670036}\n",
      "Losses {'ner': 363.38243338530884}\n",
      "Losses {'ner': 332.0183778229705}\n",
      "Losses {'ner': 295.14648551070104}\n",
      "Losses {'ner': 383.93566428041777}\n",
      "Losses {'ner': 356.2698426981062}\n",
      "Losses {'ner': 295.42530312542743}\n",
      "Losses {'ner': 355.2689411192012}\n",
      "Losses {'ner': 280.1901691876151}\n",
      "Losses {'ner': 337.4400876908485}\n",
      "Losses {'ner': 302.7367276629434}\n",
      "Losses {'ner': 315.0075960033351}\n",
      "Losses {'ner': 337.4488611726101}\n",
      "Losses {'ner': 270.8904908851715}\n",
      "Losses {'ner': 296.3958059266281}\n",
      "Losses {'ner': 307.89136053774297}\n",
      "Losses {'ner': 321.43547938023204}\n",
      "Losses {'ner': 295.8888734942917}\n",
      "Losses {'ner': 304.22337832737037}\n",
      "Losses {'ner': 313.19311820789386}\n",
      "Losses {'ner': 339.26560490001094}\n",
      "Losses {'ner': 238.72062817717426}\n",
      "Losses {'ner': 329.45572913884587}\n",
      "Losses {'ner': 329.90065842253733}\n",
      "Losses {'ner': 274.95966757368103}\n",
      "Losses {'ner': 276.40118197935567}\n",
      "Losses {'ner': 217.54983487476585}\n",
      "Losses {'ner': 291.8918635886424}\n",
      "Losses {'ner': 303.7460954970343}\n",
      "Losses {'ner': 247.1909349829504}\n",
      "Losses {'ner': 267.42976175128376}\n",
      "Losses {'ner': 285.42029871026307}\n",
      "Losses {'ner': 243.59194828586527}\n",
      "Losses {'ner': 270.3741069503299}\n",
      "Losses {'ner': 275.5913292128917}\n",
      "Losses {'ner': 279.27500078567726}\n",
      "Losses {'ner': 309.74907022804814}\n",
      "Losses {'ner': 282.3942799373873}\n",
      "Losses {'ner': 225.0056166010621}\n",
      "Losses {'ner': 220.40737004358152}\n",
      "Losses {'ner': 226.1397194593407}\n",
      "Losses {'ner': 295.3621460857432}\n",
      "Losses {'ner': 279.13925191110803}\n",
      "Losses {'ner': 247.20417695973774}\n",
      "Losses {'ner': 263.74045620858215}\n",
      "Losses {'ner': 231.4169367057536}\n",
      "Losses {'ner': 230.13544944631985}\n",
      "Losses {'ner': 205.58904585760195}\n",
      "Losses {'ner': 245.2777375191745}\n",
      "Losses {'ner': 298.7606143817398}\n",
      "Losses {'ner': 312.0777633074516}\n",
      "Losses {'ner': 250.5927966895738}\n",
      "Losses {'ner': 246.07839423685573}\n",
      "Losses {'ner': 241.40945403733272}\n",
      "Losses {'ner': 243.54439797673643}\n",
      "Losses {'ner': 289.99587023217197}\n",
      "Losses {'ner': 235.99200253688264}\n",
      "Losses {'ner': 300.7069730648731}\n",
      "Losses {'ner': 230.10507720340556}\n",
      "Losses {'ner': 206.64414567128844}\n",
      "Losses {'ner': 245.17402785088927}\n",
      "Losses {'ner': 251.70700743487626}\n",
      "Losses {'ner': 225.08052771736703}\n",
      "Losses {'ner': 242.91951400230292}\n",
      "Losses {'ner': 252.80515079741602}\n",
      "Losses {'ner': 251.14945191482198}\n",
      "Losses {'ner': 218.8624516271628}\n",
      "Losses {'ner': 245.9991281958749}\n",
      "Losses {'ner': 210.02708215740518}\n",
      "Losses {'ner': 220.66893694622297}\n",
      "Losses {'ner': 212.57844094910482}\n",
      "Losses {'ner': 203.07730674330344}\n",
      "Losses {'ner': 200.28807283690583}\n",
      "Losses {'ner': 199.08675675738908}\n",
      "Losses {'ner': 218.49418980136522}\n",
      "Losses {'ner': 221.5813632596295}\n",
      "Losses {'ner': 227.3046614129375}\n",
      "Losses {'ner': 208.87873155590222}\n",
      "Losses {'ner': 209.11127318820388}\n",
      "Losses {'ner': 177.84250202627794}\n",
      "Losses {'ner': 251.51907233933323}\n",
      "Losses {'ner': 197.63779708104533}\n",
      "Losses {'ner': 186.9384228304905}\n",
      "Losses {'ner': 216.32499194881655}\n",
      "Losses {'ner': 226.93934745329523}\n",
      "Losses {'ner': 148.35535025322318}\n",
      "Losses {'ner': 221.2735012631577}\n",
      "Losses {'ner': 177.62136266725733}\n",
      "Losses {'ner': 166.2130840190849}\n",
      "Losses {'ner': 193.33595547472592}\n",
      "Losses {'ner': 167.22114696117197}\n",
      "Losses {'ner': 239.8499579887226}\n",
      "Losses {'ner': 210.50365181647348}\n",
      "Losses {'ner': 190.97438589117016}\n",
      "Losses {'ner': 224.19882333634155}\n",
      "Losses {'ner': 213.5198688446466}\n",
      "Losses {'ner': 187.50127894766555}\n",
      "Losses {'ner': 223.19326167920747}\n",
      "Losses {'ner': 176.32248816209315}\n",
      "Losses {'ner': 221.91978597855072}\n",
      "Losses {'ner': 234.93005974119455}\n",
      "Losses {'ner': 181.52365347864293}\n",
      "Losses {'ner': 191.44542634593955}\n",
      "Losses {'ner': 214.39901164508083}\n",
      "Losses {'ner': 174.40500795831525}\n",
      "Losses {'ner': 167.7166142647215}\n",
      "Losses {'ner': 195.9463342368075}\n",
      "Losses {'ner': 184.23000533706528}\n",
      "Losses {'ner': 227.35906597757943}\n",
      "Losses {'ner': 204.65424721768025}\n",
      "Losses {'ner': 195.5497102963147}\n",
      "Losses {'ner': 173.16144768528383}\n",
      "Losses {'ner': 164.0913159551544}\n",
      "Losses {'ner': 190.56262849013228}\n",
      "Losses {'ner': 150.03366994556077}\n",
      "Losses {'ner': 187.50943707787204}\n",
      "Losses {'ner': 172.2497301583955}\n",
      "Losses {'ner': 221.72221781399634}\n",
      "Losses {'ner': 217.63115785141585}\n",
      "Losses {'ner': 163.01318211611365}\n",
      "Losses {'ner': 138.96708630616212}\n",
      "Losses {'ner': 187.22291622456913}\n",
      "Losses {'ner': 141.96045374200517}\n",
      "Losses {'ner': 180.17715897454914}\n",
      "Losses {'ner': 182.6177310435381}\n",
      "Losses {'ner': 228.47942334415004}\n",
      "Losses {'ner': 215.67909814051728}\n",
      "Losses {'ner': 174.2779807632336}\n",
      "Losses {'ner': 175.9093736219288}\n",
      "Losses {'ner': 169.31761155894486}\n",
      "Losses {'ner': 182.69215027776622}\n",
      "Losses {'ner': 178.77176222019506}\n",
      "Losses {'ner': 205.73988456589203}\n",
      "Losses {'ner': 158.25865952813623}\n",
      "Losses {'ner': 140.19538903074297}\n",
      "Losses {'ner': 178.76072317581867}\n",
      "Losses {'ner': 184.38171742643425}\n",
      "Losses {'ner': 176.23111331216546}\n",
      "Losses {'ner': 127.10220853713881}\n",
      "Losses {'ner': 162.91293656994168}\n",
      "Losses {'ner': 165.78690417173215}\n",
      "Losses {'ner': 193.97287935873265}\n",
      "Losses {'ner': 146.2378101881817}\n",
      "Losses {'ner': 219.04235051790158}\n",
      "Losses {'ner': 176.17934807307307}\n",
      "Losses {'ner': 191.58477476934914}\n",
      "Losses {'ner': 188.27347720216014}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 209.49391273926227}\n",
      "Losses {'ner': 172.65577178383418}\n",
      "Losses {'ner': 190.90588602501936}\n",
      "Losses {'ner': 177.30283806329953}\n",
      "Losses {'ner': 150.20259574718398}\n",
      "Losses {'ner': 179.0246873233989}\n",
      "Losses {'ner': 193.2881553555283}\n",
      "Losses {'ner': 188.22259838858122}\n",
      "Losses {'ner': 153.96752538473368}\n",
      "Losses {'ner': 172.39764563282185}\n",
      "Losses {'ner': 199.79950392376722}\n",
      "Losses {'ner': 197.11243844142058}\n",
      "Losses {'ner': 176.49807230840003}\n",
      "Losses {'ner': 164.72091888674817}\n",
      "Losses {'ner': 150.44791108120833}\n",
      "Losses {'ner': 173.43219515279063}\n",
      "Losses {'ner': 197.56154646925242}\n",
      "Losses {'ner': 131.2997177000286}\n",
      "Losses {'ner': 142.36527790566132}\n",
      "Losses {'ner': 136.9460709503694}\n",
      "Losses {'ner': 202.70445746312657}\n",
      "Losses {'ner': 191.56467712443964}\n",
      "Losses {'ner': 142.97334103837338}\n",
      "Losses {'ner': 174.55467383529717}\n",
      "Losses {'ner': 204.66076371927772}\n",
      "Losses {'ner': 159.8692621450786}\n",
      "Losses {'ner': 188.12953316686938}\n",
      "Losses {'ner': 149.02563853911414}\n",
      "Losses {'ner': 158.4674952972603}\n",
      "Losses {'ner': 105.64126708539769}\n",
      "Losses {'ner': 151.67962232254956}\n",
      "Losses {'ner': 135.07843163718303}\n",
      "Losses {'ner': 199.15616122729838}\n",
      "Losses {'ner': 200.93442078814}\n",
      "Losses {'ner': 202.62248890393965}\n",
      "Losses {'ner': 187.04196345423983}\n",
      "Losses {'ner': 162.81987138986554}\n",
      "Losses {'ner': 159.06730166706282}\n",
      "Losses {'ner': 178.8810222340463}\n",
      "Losses {'ner': 140.84679585121336}\n",
      "Losses {'ner': 192.6726991090408}\n",
      "Losses {'ner': 156.6983352764065}\n",
      "Losses {'ner': 156.5373376945957}\n",
      "Losses {'ner': 184.5195474555098}\n",
      "Losses {'ner': 185.47707598726157}\n",
      "Losses {'ner': 133.74882510750655}\n",
      "Losses {'ner': 136.51411500996414}\n",
      "Losses {'ner': 157.77995488216467}\n",
      "Losses {'ner': 135.2031119280722}\n",
      "Losses {'ner': 146.96063790686736}\n",
      "Losses {'ner': 148.75025763912325}\n",
      "Losses {'ner': 144.99498149346473}\n",
      "Losses {'ner': 146.19267465429445}\n",
      "Losses {'ner': 188.65309387632595}\n",
      "Losses {'ner': 145.33979189815923}\n",
      "Losses {'ner': 184.85905415772345}\n",
      "Losses {'ner': 177.2701997967114}\n",
      "Losses {'ner': 147.0722621501652}\n",
      "Losses {'ner': 178.87658395561803}\n",
      "Losses {'ner': 206.46543196587382}\n",
      "Losses {'ner': 185.05072554980643}\n",
      "Losses {'ner': 165.82562022701882}\n",
      "Losses {'ner': 144.86326774661535}\n",
      "Losses {'ner': 172.45944429296242}\n",
      "Entities in 'Do you like horses?'\n",
      "Saved model to Spacy_Model\n",
      "Loading from Spacy_Model\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the model\n",
    "### STEP1: Reading a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"Test/Doc1.json\",\"r\") as read_file:\n",
    "    data = json.load(read_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP2: Processing the json to get csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original\n",
    "myData=[]\n",
    "for document in data:\n",
    "    text=document['text']\n",
    "    H0=document['H0']\n",
    "    H1=document['H1']\n",
    "    i=0\n",
    "    sentence=\"\"\n",
    "    categ=\"None\"\n",
    "    entities=[]\n",
    "    while(i<len(text)):\n",
    "        if(len(sentence)==0):\n",
    "            start_index=0\n",
    "        else:\n",
    "            start_index=len(sentence)\n",
    "        if(H0[i]==1):\n",
    "            entities.append([start_index,start_index+len(text[i])-1,\"H0\"])\n",
    "            if(categ!=\"H0\"):\n",
    "                #categ='Priv'\n",
    "                categ='H0'\n",
    "        if(H1[i]==1 and H0[i]==0):\n",
    "            entities.append([start_index,start_index+len(text[i]),\"H1\"])\n",
    "            if(categ!=\"H1\" and categ!=\"H0\"):\n",
    "                #categ='Priv'\n",
    "                categ='H1'\n",
    "        sentence=sentence+text[i]+\" \"\n",
    "        if('.' in text[i]):\n",
    "            myData.append([sentence,categ,entities])\n",
    "            sentence=\"\"\n",
    "            categ=\"None\"\n",
    "            entities=[]\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP3: Writing into a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "my_df = pd.DataFrame(myData)\n",
    "my_df.columns=['Sentence', 'Category', 'Spacy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.to_csv('Test/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Spacy to find entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Spacy inbuilt model and our model\n",
    "output_dir=\"Spacy_Model\"\n",
    "nlp2 = spacy.load(output_dir)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import string\n",
    "table = str.maketrans(dict.fromkeys(string.punctuation)) #Used to remove the punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy prebuilt model's entities\n",
    "#NORP: Nationalities or religious or political groups. ---- H1\n",
    "#PERSON: People, including fictional ------ H0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the csv file\n",
    "data = pd.read_csv(\"Test/test.csv\", index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output=[]\n",
    "dictText={}\n",
    "dictText['text']=[]\n",
    "dictText['H0']=[]\n",
    "dictText['H1']=[]\n",
    "for index, row in data.iterrows():\n",
    "    sent_mymodel = nlp2(row['Sentence'])\n",
    "    sent_inbuilt=nlp(row['Sentence'])\n",
    "    entities=[]\n",
    "    for ent in sent_inbuilt.ents:\n",
    "        if(ent.label_=='PERSON'):\n",
    "            #print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "            entities.append([ent,'H0'])\n",
    "        elif( ent.label_=='NORP'):\n",
    "            entities.append([ent,'H1'])\n",
    "            #print([ent,'H1'])\n",
    "    for ent in sent_mymodel.ents:\n",
    "        #print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "        entities.append([ent,ent.label_])\n",
    "    #print(row['Sentence'])\n",
    "    words= row['Sentence'].split(\" \")\n",
    "    for word in words:\n",
    "        dictText['text'].append(word)\n",
    "        entity_name=None\n",
    "        #count=0\n",
    "        for entity in entities:\n",
    "            #print(word.translate(table))\n",
    "            #print(entity[0])\n",
    "            #print(word.translate(table) in str(entity[0]))\n",
    "            #print(\"For word:{0}, check entity:{1}, status:{2}\".format(word, entity[0], word.translate(table) in str(entity[0])))\n",
    "            if(word.translate(table) in str(entity[0])):\n",
    "                entity_name=str(entity[1])\n",
    "                #print(\"Word:{0}, EntityName:{1}\".format(word,entity[1])) \n",
    "                break\n",
    "                \n",
    "        if(entity_name!=None):\n",
    "            if(entity_name=='H0'):\n",
    "                dictText['H0'].append(1)\n",
    "            else:\n",
    "                dictText['H0'].append(0)\n",
    "            dictText['H1'].append(1)\n",
    "        else:\n",
    "            dictText['H0'].append(0)\n",
    "            dictText['H1'].append(0)\n",
    "        \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing json\n",
    "import json\n",
    "with open('Test/Output.json', 'w') as outfile:\n",
    "    json.dump(dictText, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Predict import outputJson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-c6b273904980>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moutputJson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"D:/Berkeley/1stSem/290/Project/Privacy_redaction/Test/Doc1.json\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"D:/Berkeley/1stSem/290/Project/Privacy_redaction/Test/Doc1_output.json\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Berkeley\\1stSem\\290\\Project\\Privacy_redaction\\Predict.py\u001b[0m in \u001b[0;36moutputJson\u001b[1;34m(input_file_path, output_file_path)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[0moutput_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Spacy_Model\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m     \u001b[0mnlp2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"en_core_web_sm\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spacy' is not defined"
     ]
    }
   ],
   "source": [
    "outputJson(\"D:/Berkeley/1stSem/290/Project/Privacy_redaction/Test/Doc1.json\",\"D:/Berkeley/1stSem/290/Project/Privacy_redaction/Test/Doc1_output.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
