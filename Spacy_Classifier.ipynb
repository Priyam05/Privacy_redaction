{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building NER model to identify the entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = [\n",
    "    (\"Who is Shaka Khan?\", {\"entities\": [(7, 17, \"H0\")]}),\n",
    "    (\"I like London and Berlin.\", {\"entities\": [(7, 13, \"H1\"), (18, 24, \"H0\")]}),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"sent_classifier_train.csv\", index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing Training Data for Spacy\n",
    "#table = str.maketrans('[]','')\n",
    "TRAIN_DATA=[]\n",
    "for index, row in data.iterrows():\n",
    "    #print(row['Spacy'])\n",
    "    entities_dict={}\n",
    "    \n",
    "    sentence=row['Sentence']\n",
    "    entities=row['Spacy'].split(\"],\")\n",
    "    entities_list=[]\n",
    "    if(len(row['Spacy'].strip())>2):\n",
    "        for entity_str in entities:\n",
    "            #entity.translate(table)\n",
    "            req_entity_str=entity_str.replace('[','').replace(']','')\n",
    "            req_entity=req_entity_str.split(',')\n",
    "            start_index=int(req_entity[0].strip())\n",
    "            #print(start_index)\n",
    "            end_index=int(req_entity[1].strip())\n",
    "            #print(end_index)\n",
    "            entity=req_entity[2].strip().replace(\"'\",\"\")\n",
    "            #print(entity)\n",
    "            entities_list.append((start_index,end_index,entity))\n",
    "    entities_dict['entities']=entities_list\n",
    "    TRAIN_DATA.append((row['Sentence'],entities_dict))\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Isaac David Abella (June 20, 1934 â€“ October 23, 2016) was a Professor of Physics at The University of Chicago. ',\n",
       " {'entities': [(0, 4, 'H0'),\n",
       "   (6, 10, 'H0'),\n",
       "   (12, 17, 'H0'),\n",
       "   (25, 28, 'H1'),\n",
       "   (29, 33, 'H1'),\n",
       "   (34, 35, 'H1'),\n",
       "   (36, 43, 'H1'),\n",
       "   (44, 47, 'H1'),\n",
       "   (48, 53, 'H1'),\n",
       "   (54, 57, 'H1'),\n",
       "   (60, 69, 'H1'),\n",
       "   (70, 72, 'H1'),\n",
       "   (73, 80, 'H1'),\n",
       "   (81, 83, 'H1'),\n",
       "   (84, 87, 'H1'),\n",
       "   (88, 98, 'H1'),\n",
       "   (99, 101, 'H1'),\n",
       "   (102, 110, 'H1')]})"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DATA[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for Spacy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-59-34251adb9d5b>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-59-34251adb9d5b>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    )\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "@plac.annotations(\n",
    "    model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n",
    "    new_model_name=(\"New model name for model meta.\", \"option\", \"nm\", str),\n",
    "    output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n",
    "    n_iter=(\"Number of training iterations\", \"option\", \"n\", int),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model=None, new_model_name=\"Privacy_Redaction\", output_dir=\"Spacy_Model\", n_iter=100):\n",
    "    \"\"\"Set up the pipeline and entity recognizer, and train the new entity.\"\"\"\n",
    "    random.seed(0)\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "    # Add entity recognizer to model if it's not in the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner)\n",
    "    # otherwise, get it, so we can add labels to it\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "        \n",
    "    #Add H0 and H1 as entities\n",
    "    ner.add_label(\"H0\") \n",
    "    ner.add_label(\"H1\") \n",
    "    \n",
    "    if model is None:\n",
    "        optimizer = nlp.begin_training()\n",
    "    else:\n",
    "        optimizer = nlp.resume_training()\n",
    "        \n",
    "    move_names = list(ner.move_names)\n",
    "    \n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "    \n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        sizes = compounding(1.0, 4.0, 1.001)\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            batches = minibatch(TRAIN_DATA, size=sizes)\n",
    "            losses = {}\n",
    "            for batch in batches:\n",
    "                #print(batch)\n",
    "                texts, annotations = zip(*batch)\n",
    "                #print(zip(*batch))\n",
    "                #print(text)\n",
    "                #print(annotationsations)\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
    "            print(\"Losses\", losses)\n",
    "    \n",
    "    # test the trained model\n",
    "    test_text = \"Do you like horses?\"\n",
    "    doc = nlp(test_text)\n",
    "    print(\"Entities in '%s'\" % test_text)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.label_, ent.text)\n",
    "        \n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.meta[\"name\"] = new_model_name  # rename model\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        # Check the classes have loaded back consistently\n",
    "        assert nlp2.get_pipe(\"ner\").move_names == move_names\n",
    "        doc2 = nlp2(test_text)\n",
    "        for ent in doc2.ents:\n",
    "            print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n",
      "Losses {'ner': 7001.339179119419}\n",
      "Losses {'ner': 5274.288624803818}\n",
      "Losses {'ner': 4233.215686360598}\n",
      "Losses {'ner': 3708.6963201080725}\n",
      "Losses {'ner': 3470.051681057848}\n",
      "Losses {'ner': 2994.283204119511}\n",
      "Losses {'ner': 2580.1443003455224}\n",
      "Losses {'ner': 2420.2402061846124}\n",
      "Losses {'ner': 2193.045202154296}\n",
      "Losses {'ner': 2040.6577171647307}\n",
      "Losses {'ner': 1962.9975720766047}\n",
      "Losses {'ner': 1784.862245879911}\n",
      "Losses {'ner': 1586.1338208529787}\n",
      "Losses {'ner': 1539.7148191993008}\n",
      "Losses {'ner': 1516.195417077236}\n",
      "Losses {'ner': 1454.382354292505}\n",
      "Losses {'ner': 1391.2223248088128}\n",
      "Losses {'ner': 1277.1871288780574}\n",
      "Losses {'ner': 1164.4331448397218}\n",
      "Losses {'ner': 1169.8963033545901}\n",
      "Losses {'ner': 1135.1620668712903}\n",
      "Losses {'ner': 1076.4378280924664}\n",
      "Losses {'ner': 1085.606820821391}\n",
      "Losses {'ner': 1005.0197114568757}\n",
      "Losses {'ner': 1024.7630288457262}\n",
      "Losses {'ner': 991.8002231639651}\n",
      "Losses {'ner': 944.9877999481828}\n",
      "Losses {'ner': 926.660604939327}\n",
      "Losses {'ner': 912.5014241878159}\n",
      "Losses {'ner': 910.2413880979776}\n",
      "Losses {'ner': 868.4579086253227}\n",
      "Losses {'ner': 923.8939316538388}\n",
      "Losses {'ner': 849.7356330757262}\n",
      "Losses {'ner': 885.5599592773355}\n",
      "Losses {'ner': 753.9032758195749}\n",
      "Losses {'ner': 768.577425423113}\n",
      "Losses {'ner': 731.3905839728471}\n",
      "Losses {'ner': 744.6088019058727}\n",
      "Losses {'ner': 770.4268385915027}\n",
      "Losses {'ner': 690.0056241761288}\n",
      "Losses {'ner': 680.0052786218563}\n",
      "Losses {'ner': 689.6597327455994}\n",
      "Losses {'ner': 699.3265306585129}\n",
      "Losses {'ner': 705.7551310115651}\n",
      "Losses {'ner': 591.217025742919}\n",
      "Losses {'ner': 598.5256714976572}\n",
      "Losses {'ner': 619.9153801464453}\n",
      "Losses {'ner': 597.96204468338}\n",
      "Losses {'ner': 521.8133894527858}\n",
      "Losses {'ner': 621.986501826779}\n",
      "Losses {'ner': 593.3279798097128}\n",
      "Losses {'ner': 578.9400205862092}\n",
      "Losses {'ner': 502.58358718316737}\n",
      "Losses {'ner': 555.703170513972}\n",
      "Losses {'ner': 512.9686895312446}\n",
      "Losses {'ner': 451.7953903328062}\n",
      "Losses {'ner': 465.7259101558596}\n",
      "Losses {'ner': 458.58148524659646}\n",
      "Losses {'ner': 561.0547265158721}\n",
      "Losses {'ner': 431.6587228723024}\n",
      "Losses {'ner': 451.51264137653516}\n",
      "Losses {'ner': 488.0450180004966}\n",
      "Losses {'ner': 422.365615570982}\n",
      "Losses {'ner': 489.45821847073904}\n",
      "Losses {'ner': 430.73505115825634}\n",
      "Losses {'ner': 502.26366076432964}\n",
      "Losses {'ner': 495.50083131863636}\n",
      "Losses {'ner': 439.3423127237976}\n",
      "Losses {'ner': 450.40503980709195}\n",
      "Losses {'ner': 416.7513788037936}\n",
      "Losses {'ner': 327.7945796743812}\n",
      "Losses {'ner': 417.43587148430225}\n",
      "Losses {'ner': 396.4386429610618}\n",
      "Losses {'ner': 428.7301387830251}\n",
      "Losses {'ner': 404.89256624400537}\n",
      "Losses {'ner': 478.9711395102249}\n",
      "Losses {'ner': 299.26200766801}\n",
      "Losses {'ner': 413.07810780062783}\n",
      "Losses {'ner': 385.8139647962089}\n",
      "Losses {'ner': 390.21228306330977}\n",
      "Losses {'ner': 372.8712154686392}\n",
      "Losses {'ner': 390.19514158629886}\n",
      "Losses {'ner': 345.2924968033322}\n",
      "Losses {'ner': 382.6805317348945}\n",
      "Losses {'ner': 316.0262109354966}\n",
      "Losses {'ner': 390.00551658017406}\n",
      "Losses {'ner': 362.08788475537744}\n",
      "Losses {'ner': 354.9982162366656}\n",
      "Losses {'ner': 351.7412604148042}\n",
      "Losses {'ner': 331.87080439870016}\n",
      "Losses {'ner': 449.1610341636827}\n",
      "Losses {'ner': 394.19185445059844}\n",
      "Losses {'ner': 363.9879069848453}\n",
      "Losses {'ner': 330.77603544876405}\n",
      "Losses {'ner': 338.9773157749496}\n",
      "Losses {'ner': 358.8966253622601}\n",
      "Losses {'ner': 339.8809480496188}\n",
      "Losses {'ner': 267.4128910391989}\n",
      "Losses {'ner': 319.98128838749057}\n",
      "Losses {'ner': 278.6106755752658}\n",
      "Entities in 'Do you like horses?'\n",
      "Saved model to Spacy_Model\n",
      "Loading from Spacy_Model\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir=\"Spacy_Model\"\n",
    "nlp2 = spacy.load(output_dir)\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=\"He became a mugham teacher in 1973 at the Abilov Culture House in Baku, and also worked as a teacher at the Zeynally Music College in 1977.\"\n",
    "doc1 = nlp2(sample)\n",
    "doc2=nlp(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My model\n",
      "mugham 12 18 H1\n",
      "teacher 19 26 H1\n",
      "in 27 29 H1\n",
      "1973 30 34 H1\n",
      "at 35 37 H1\n",
      "the 38 41 H1\n",
      "Abilov 42 48 H1\n",
      "Culture 49 56 H1\n",
      "House 57 62 H1\n"
     ]
    }
   ],
   "source": [
    "print(\"My model\")\n",
    "for ent in doc1.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy model\n",
      "Aghakhan Abdullayev 0 19 PERSON\n",
      "Azerbaijani 21 32 NORP\n",
      "Abdullayev 42 52 PERSON\n",
      "February 1950 56 69 DATE\n",
      "25 December 2016 74 90 DATE\n",
      "Azerbaijani 99 110 NORP\n"
     ]
    }
   ],
   "source": [
    "print(\"Spacy model\")\n",
    "for ent in doc2.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=\"Abdullayev was born in Baku where he graduated from secondary school in 1968, then continued his education at the Zeynally Music College until 1973.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Baku'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[23:27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
