{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building NER model to identify the entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = [\n",
    "    (\"Who is Shaka Khan?\", {\"entities\": [(7, 17, \"H0\")]}),\n",
    "    (\"I like London and Berlin.\", {\"entities\": [(7, 13, \"H1\"), (18, 24, \"H0\")]}),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"sent_classifier_train.csv\", index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing Training Data for Spacy\n",
    "#table = str.maketrans('[]','')\n",
    "TRAIN_DATA=[]\n",
    "for index, row in data.iterrows():\n",
    "    #print(row['Spacy'])\n",
    "    entities_dict={}\n",
    "    \n",
    "    sentence=row['Sentence']\n",
    "    entities=row['Spacy'].split(\"],\")\n",
    "    entities_list=[]\n",
    "    if(len(row['Spacy'].strip())>2):\n",
    "        for entity_str in entities:\n",
    "            #entity.translate(table)\n",
    "            req_entity_str=entity_str.replace('[','').replace(']','')\n",
    "            req_entity=req_entity_str.split(',')\n",
    "            start_index=int(req_entity[0].strip())\n",
    "            #print(start_index)\n",
    "            end_index=int(req_entity[1].strip())\n",
    "            #print(end_index)\n",
    "            entity=req_entity[2].strip().replace(\"'\",\"\")\n",
    "            #print(entity)\n",
    "            entities_list.append((start_index,end_index,entity))\n",
    "    entities_dict['entities']=entities_list\n",
    "    TRAIN_DATA.append((row['Sentence'],entities_dict))\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Isaac David Abella (June 20, 1934 – October 23, 2016) was a Professor of Physics at The University of Chicago. ',\n",
       " {'entities': [(0, 4, 'H0'),\n",
       "   (6, 10, 'H0'),\n",
       "   (12, 17, 'H0'),\n",
       "   (25, 28, 'H1'),\n",
       "   (29, 33, 'H1'),\n",
       "   (34, 35, 'H1'),\n",
       "   (36, 43, 'H1'),\n",
       "   (44, 47, 'H1'),\n",
       "   (48, 53, 'H1'),\n",
       "   (54, 57, 'H1'),\n",
       "   (60, 69, 'H1'),\n",
       "   (70, 72, 'H1'),\n",
       "   (73, 80, 'H1'),\n",
       "   (81, 83, 'H1'),\n",
       "   (84, 87, 'H1'),\n",
       "   (88, 98, 'H1'),\n",
       "   (99, 101, 'H1'),\n",
       "   (102, 110, 'H1')]})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DATA[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for Spacy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-59-34251adb9d5b>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-59-34251adb9d5b>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    )\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "@plac.annotations(\n",
    "    model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n",
    "    new_model_name=(\"New model name for model meta.\", \"option\", \"nm\", str),\n",
    "    output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n",
    "    n_iter=(\"Number of training iterations\", \"option\", \"n\", int),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model=None, new_model_name=\"Privacy_Redaction\", output_dir=\"Spacy_Model\", n_iter=300):\n",
    "    \"\"\"Set up the pipeline and entity recognizer, and train the new entity.\"\"\"\n",
    "    random.seed(0)\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "    # Add entity recognizer to model if it's not in the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner)\n",
    "    # otherwise, get it, so we can add labels to it\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "        \n",
    "    #Add H0 and H1 as entities\n",
    "    ner.add_label(\"H0\") \n",
    "    ner.add_label(\"H1\") \n",
    "    \n",
    "    if model is None:\n",
    "        optimizer = nlp.begin_training()\n",
    "    else:\n",
    "        optimizer = nlp.resume_training()\n",
    "        \n",
    "    move_names = list(ner.move_names)\n",
    "    \n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "    \n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        sizes = compounding(1.0, 4.0, 1.001)\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            batches = minibatch(TRAIN_DATA, size=sizes)\n",
    "            losses = {}\n",
    "            for batch in batches:\n",
    "                #print(batch)\n",
    "                texts, annotations = zip(*batch)\n",
    "                #print(zip(*batch))\n",
    "                #print(text)\n",
    "                #print(annotationsations)\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
    "            print(\"Losses\", losses)\n",
    "    \n",
    "    # test the trained model\n",
    "    test_text = \"Do you like horses?\"\n",
    "    doc = nlp(test_text)\n",
    "    print(\"Entities in '%s'\" % test_text)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.label_, ent.text)\n",
    "        \n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.meta[\"name\"] = new_model_name  # rename model\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        # Check the classes have loaded back consistently\n",
    "        assert nlp2.get_pipe(\"ner\").move_names == move_names\n",
    "        doc2 = nlp2(test_text)\n",
    "        for ent in doc2.ents:\n",
    "            print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n",
      "Losses {'ner': 6984.608969391234}\n",
      "Losses {'ner': 5149.077116403521}\n",
      "Losses {'ner': 4378.090314007212}\n",
      "Losses {'ner': 3734.79931748015}\n",
      "Losses {'ner': 3281.2561587755295}\n",
      "Losses {'ner': 3004.867279932058}\n",
      "Losses {'ner': 2696.5801871788663}\n",
      "Losses {'ner': 2399.5476219243337}\n",
      "Losses {'ner': 2269.5227938266244}\n",
      "Losses {'ner': 2151.233255962891}\n",
      "Losses {'ner': 1967.4936714395074}\n",
      "Losses {'ner': 1778.8185092237288}\n",
      "Losses {'ner': 1739.3674873866144}\n",
      "Losses {'ner': 1566.7262238851197}\n",
      "Losses {'ner': 1499.9920394995888}\n",
      "Losses {'ner': 1438.9841819992075}\n",
      "Losses {'ner': 1406.8210648244315}\n",
      "Losses {'ner': 1246.8125117346779}\n",
      "Losses {'ner': 1354.6964409755158}\n",
      "Losses {'ner': 1279.0518894672423}\n",
      "Losses {'ner': 1151.19263188842}\n",
      "Losses {'ner': 1082.1056158472381}\n",
      "Losses {'ner': 1126.5109987091773}\n",
      "Losses {'ner': 1027.8371648012994}\n",
      "Losses {'ner': 974.516844457925}\n",
      "Losses {'ner': 1083.5228720340042}\n",
      "Losses {'ner': 921.4054606219256}\n",
      "Losses {'ner': 913.197112164008}\n",
      "Losses {'ner': 830.1560100584119}\n",
      "Losses {'ner': 864.4408725349383}\n",
      "Losses {'ner': 862.9981591611792}\n",
      "Losses {'ner': 852.0886639829623}\n",
      "Losses {'ner': 819.8227460223981}\n",
      "Losses {'ner': 848.8401068931752}\n",
      "Losses {'ner': 753.9388225016512}\n",
      "Losses {'ner': 746.3800441403988}\n",
      "Losses {'ner': 730.695120434991}\n",
      "Losses {'ner': 704.5437804251577}\n",
      "Losses {'ner': 713.4977407962486}\n",
      "Losses {'ner': 671.3099945725936}\n",
      "Losses {'ner': 732.9383242410559}\n",
      "Losses {'ner': 695.9045326200759}\n",
      "Losses {'ner': 615.1447637656717}\n",
      "Losses {'ner': 672.5051665322737}\n",
      "Losses {'ner': 615.5391575278609}\n",
      "Losses {'ner': 622.8688331751382}\n",
      "Losses {'ner': 619.9364124971725}\n",
      "Losses {'ner': 605.0850686230208}\n",
      "Losses {'ner': 597.5332944308024}\n",
      "Losses {'ner': 556.2557157292746}\n",
      "Losses {'ner': 619.2625473806263}\n",
      "Losses {'ner': 553.8651147479811}\n",
      "Losses {'ner': 566.7108428880291}\n",
      "Losses {'ner': 499.2898770606351}\n",
      "Losses {'ner': 577.8438761182248}\n",
      "Losses {'ner': 489.1502368803732}\n",
      "Losses {'ner': 518.641645749797}\n",
      "Losses {'ner': 506.5398162347616}\n",
      "Losses {'ner': 518.6500319157974}\n",
      "Losses {'ner': 515.7176270258029}\n",
      "Losses {'ner': 506.33964989381866}\n",
      "Losses {'ner': 479.84748851221354}\n",
      "Losses {'ner': 520.9662266493855}\n",
      "Losses {'ner': 528.1009322717987}\n",
      "Losses {'ner': 463.62410550676873}\n",
      "Losses {'ner': 396.25697683452125}\n",
      "Losses {'ner': 430.4432000239474}\n",
      "Losses {'ner': 433.84859783024973}\n",
      "Losses {'ner': 499.54557929601094}\n",
      "Losses {'ner': 432.92492655607714}\n",
      "Losses {'ner': 376.78582945028194}\n",
      "Losses {'ner': 435.42392674854534}\n",
      "Losses {'ner': 354.1119739758393}\n",
      "Losses {'ner': 438.21683928689254}\n",
      "Losses {'ner': 395.21625524149624}\n",
      "Losses {'ner': 377.4363507127056}\n",
      "Losses {'ner': 352.4740327812021}\n",
      "Losses {'ner': 452.42518361221977}\n",
      "Losses {'ner': 372.04105218057015}\n",
      "Losses {'ner': 326.66187495860555}\n",
      "Losses {'ner': 419.34857001266874}\n",
      "Losses {'ner': 363.65566018843685}\n",
      "Losses {'ner': 372.4334442439468}\n",
      "Losses {'ner': 356.32464366612265}\n",
      "Losses {'ner': 306.7332257006389}\n",
      "Losses {'ner': 349.7515812837836}\n",
      "Losses {'ner': 363.66116430711116}\n",
      "Losses {'ner': 402.4020378986042}\n",
      "Losses {'ner': 365.1299689241504}\n",
      "Losses {'ner': 311.0519140670036}\n",
      "Losses {'ner': 363.38243338530884}\n",
      "Losses {'ner': 332.0183778229705}\n",
      "Losses {'ner': 295.14648551070104}\n",
      "Losses {'ner': 383.93566428041777}\n",
      "Losses {'ner': 356.2698426981062}\n",
      "Losses {'ner': 295.42530312542743}\n",
      "Losses {'ner': 355.2689411192012}\n",
      "Losses {'ner': 280.1901691876151}\n",
      "Losses {'ner': 337.4400876908485}\n",
      "Losses {'ner': 302.7367276629434}\n",
      "Losses {'ner': 315.0075960033351}\n",
      "Losses {'ner': 337.4488611726101}\n",
      "Losses {'ner': 270.8904908851715}\n",
      "Losses {'ner': 296.3958059266281}\n",
      "Losses {'ner': 307.89136053774297}\n",
      "Losses {'ner': 321.43547938023204}\n",
      "Losses {'ner': 295.8888734942917}\n",
      "Losses {'ner': 304.22337832737037}\n",
      "Losses {'ner': 313.19311820789386}\n",
      "Losses {'ner': 339.26560490001094}\n",
      "Losses {'ner': 238.72062817717426}\n",
      "Losses {'ner': 329.45572913884587}\n",
      "Losses {'ner': 329.90065842253733}\n",
      "Losses {'ner': 274.95966757368103}\n",
      "Losses {'ner': 276.40118197935567}\n",
      "Losses {'ner': 217.54983487476585}\n",
      "Losses {'ner': 291.8918635886424}\n",
      "Losses {'ner': 303.7460954970343}\n",
      "Losses {'ner': 247.1909349829504}\n",
      "Losses {'ner': 267.42976175128376}\n",
      "Losses {'ner': 285.42029871026307}\n",
      "Losses {'ner': 243.59194828586527}\n",
      "Losses {'ner': 270.3741069503299}\n",
      "Losses {'ner': 275.5913292128917}\n",
      "Losses {'ner': 279.27500078567726}\n",
      "Losses {'ner': 309.74907022804814}\n",
      "Losses {'ner': 282.3942799373873}\n",
      "Losses {'ner': 225.0056166010621}\n",
      "Losses {'ner': 220.40737004358152}\n",
      "Losses {'ner': 226.1397194593407}\n",
      "Losses {'ner': 295.3621460857432}\n",
      "Losses {'ner': 279.13925191110803}\n",
      "Losses {'ner': 247.20417695973774}\n",
      "Losses {'ner': 263.74045620858215}\n",
      "Losses {'ner': 231.4169367057536}\n",
      "Losses {'ner': 230.13544944631985}\n",
      "Losses {'ner': 205.58904585760195}\n",
      "Losses {'ner': 245.2777375191745}\n",
      "Losses {'ner': 298.7606143817398}\n",
      "Losses {'ner': 312.0777633074516}\n",
      "Losses {'ner': 250.5927966895738}\n",
      "Losses {'ner': 246.07839423685573}\n",
      "Losses {'ner': 241.40945403733272}\n",
      "Losses {'ner': 243.54439797673643}\n",
      "Losses {'ner': 289.99587023217197}\n",
      "Losses {'ner': 235.99200253688264}\n",
      "Losses {'ner': 300.7069730648731}\n",
      "Losses {'ner': 230.10507720340556}\n",
      "Losses {'ner': 206.64414567128844}\n",
      "Losses {'ner': 245.17402785088927}\n",
      "Losses {'ner': 251.70700743487626}\n",
      "Losses {'ner': 225.08052771736703}\n",
      "Losses {'ner': 242.91951400230292}\n",
      "Losses {'ner': 252.80515079741602}\n",
      "Losses {'ner': 251.14945191482198}\n",
      "Losses {'ner': 218.8624516271628}\n",
      "Losses {'ner': 245.9991281958749}\n",
      "Losses {'ner': 210.02708215740518}\n",
      "Losses {'ner': 220.66893694622297}\n",
      "Losses {'ner': 212.57844094910482}\n",
      "Losses {'ner': 203.07730674330344}\n",
      "Losses {'ner': 200.28807283690583}\n",
      "Losses {'ner': 199.08675675738908}\n",
      "Losses {'ner': 218.49418980136522}\n",
      "Losses {'ner': 221.5813632596295}\n",
      "Losses {'ner': 227.3046614129375}\n",
      "Losses {'ner': 208.87873155590222}\n",
      "Losses {'ner': 209.11127318820388}\n",
      "Losses {'ner': 177.84250202627794}\n",
      "Losses {'ner': 251.51907233933323}\n",
      "Losses {'ner': 197.63779708104533}\n",
      "Losses {'ner': 186.9384228304905}\n",
      "Losses {'ner': 216.32499194881655}\n",
      "Losses {'ner': 226.93934745329523}\n",
      "Losses {'ner': 148.35535025322318}\n",
      "Losses {'ner': 221.2735012631577}\n",
      "Losses {'ner': 177.62136266725733}\n",
      "Losses {'ner': 166.2130840190849}\n",
      "Losses {'ner': 193.33595547472592}\n",
      "Losses {'ner': 167.22114696117197}\n",
      "Losses {'ner': 239.8499579887226}\n",
      "Losses {'ner': 210.50365181647348}\n",
      "Losses {'ner': 190.97438589117016}\n",
      "Losses {'ner': 224.19882333634155}\n",
      "Losses {'ner': 213.5198688446466}\n",
      "Losses {'ner': 187.50127894766555}\n",
      "Losses {'ner': 223.19326167920747}\n",
      "Losses {'ner': 176.32248816209315}\n",
      "Losses {'ner': 221.91978597855072}\n",
      "Losses {'ner': 234.93005974119455}\n",
      "Losses {'ner': 181.52365347864293}\n",
      "Losses {'ner': 191.44542634593955}\n",
      "Losses {'ner': 214.39901164508083}\n",
      "Losses {'ner': 174.40500795831525}\n",
      "Losses {'ner': 167.7166142647215}\n",
      "Losses {'ner': 195.9463342368075}\n",
      "Losses {'ner': 184.23000533706528}\n",
      "Losses {'ner': 227.35906597757943}\n",
      "Losses {'ner': 204.65424721768025}\n",
      "Losses {'ner': 195.5497102963147}\n",
      "Losses {'ner': 173.16144768528383}\n",
      "Losses {'ner': 164.0913159551544}\n",
      "Losses {'ner': 190.56262849013228}\n",
      "Losses {'ner': 150.03366994556077}\n",
      "Losses {'ner': 187.50943707787204}\n",
      "Losses {'ner': 172.2497301583955}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on a csv file and generating json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir=\"Spacy_Model\"\n",
    "nlp2 = spacy.load(output_dir)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import string\n",
    "table = str.maketrans(dict.fromkeys(string.punctuation)) #Used to remove the punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy prebuilt model's entities\n",
    "#NORP: Nationalities or religious or political groups. ---- H1\n",
    "#PERSON: People, including fictional ------ H0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the csv file\n",
    "data = pd.read_csv(\"sent_classifier_train.csv\", index_col=False)\n",
    "nlp2 = spacy.load(output_dir)\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D., 'H1']\n",
      "[D., 'H1']\n",
      "[D., 'H1']\n",
      "[D., 'H1']\n",
      "[Azerbaijani, 'H1']\n",
      "[Azerbaijani, 'H1']\n",
      "[Nigerian, 'H1']\n",
      "[Bengali, 'H1']\n",
      "[Islamic, 'H1']\n",
      "[Islamic, 'H1']\n",
      "[ইসলাম, 'H1']\n",
      "[Muslim, 'H1']\n",
      "[Indian, 'H1']\n",
      "[Indian, 'H1']\n",
      "[Islamic, 'H1']\n",
      "[Islamic, 'H1']\n",
      "[Islamic, 'H1']\n",
      "[Islamic, 'H1']\n",
      "[Qatari, 'H1']\n",
      "[Senegalese, 'H1']\n",
      "[Somali, 'H1']\n",
      "[Somali American, 'H1']\n",
      "[Arabic, 'H1']\n",
      "[Rabat, 'H1']\n",
      "[Moroccan, 'H1']\n",
      "[Arab, 'H1']\n",
      "[Spanish, 'H1']\n",
      "[Moroccan, 'H1']\n",
      "[Moroccan, 'H1']\n",
      "[Arabic, 'H1']\n",
      "[Egyptian, 'H1']\n",
      "[Arab, 'H1']\n",
      "[Egyptian, 'H1']\n",
      "[Egyptian, 'H1']\n",
      "[Egyptian, 'H1']\n",
      "[Egyptian, 'H1']\n",
      "[Egyptian, 'H1']\n",
      "[Egyptian, 'H1']\n",
      "[American, 'H1']\n",
      "[Costello, 'H1']\n",
      "[Sudanese, 'H1']\n",
      "[Ghanaian, 'H1']\n",
      "[Christian, 'H1']\n",
      "[Ghanaian, 'H1']\n",
      "[Norwegian, 'H1']\n",
      "[Norwegian, 'H1']\n",
      "[Norwegian, 'H1']\n",
      "[Swedish, 'H1']\n",
      "[Norwegian, 'H1']\n",
      "[Indian, 'H1']\n",
      "[American, 'H1']\n",
      "[Jewish, 'H1']\n",
      "[American, 'H1']\n",
      "[Americanist, 'H1']\n",
      "[American, 'H1']\n",
      "[American, 'H1']\n",
      "[American, 'H1']\n",
      "[American, 'H1']\n",
      "[American, 'H1']\n",
      "[American, 'H1']\n",
      "[American, 'H1']\n",
      "[Norwegian, 'H1']\n",
      "[Norwegian, 'H1']\n",
      "[American, 'H1']\n",
      "[Ghanaian, 'H1']\n",
      "[North American, 'H1']\n",
      "[African, 'H1']\n",
      "[Indonesian, 'H1']\n",
      "[West Sumatra, 'H1']\n",
      "[West Sumatra, 'H1']\n",
      "[Salvadoran, 'H1']\n",
      "[Argentine, 'H1']\n",
      "[American, 'H1']\n",
      "[American, 'H1']\n",
      "[Democrat, 'H1']\n",
      "[Persian, 'H1']\n",
      "[Iranian, 'H1']\n",
      "[Salmas, 'H1']\n",
      "[Salmas, 'H1']\n",
      "[Japanese, 'H1']\n",
      "[Japanese, 'H1']\n",
      "[Japanese, 'H1']\n",
      "[Chinese, 'H1']\n",
      "[Indian-American, 'H1']\n",
      "[non-Telugu, 'H1']\n",
      "[Indian, 'H1']\n",
      "[Hindu, 'H1']\n",
      "[Indian, 'H1']\n",
      "[Swiss, 'H1']\n",
      "[Parisian, 'H1']\n",
      "[German, 'H1']\n",
      "[French, 'H1']\n",
      "[Italian, 'H1']\n",
      "[American, 'H1']\n",
      "[Canadian, 'H1']\n",
      "[Adrian, 'H1']\n",
      "[Adrian, 'H1']\n",
      "[Adrian, 'H1']\n",
      "[Austrian, 'H1']\n",
      "[Adrian, 'H1']\n",
      "[German, 'H1']\n",
      "[Turkish, 'H1']\n",
      "[Syrian, 'H1']\n",
      "[Italian, 'H1']\n",
      "[American, 'H1']\n",
      "[Titans, 'H1']\n",
      "[Maharashtra, 'H1']\n",
      "[American, 'H1']\n",
      "[Jewish, 'H1']\n",
      "[Nigerian, 'H1']\n",
      "[West African, 'H1']\n",
      "[Nigeria, 'H1']\n",
      "[D., 'H1']\n",
      "[American, 'H1']\n",
      "[Canadian, 'H1']\n",
      "[Australian, 'H1']\n",
      "[Turkish, 'H1']\n",
      "[American, 'H1']\n",
      "[D., 'H1']\n",
      "[D., 'H1']\n",
      "[Adamian, 'H1']\n",
      "[Armenian, 'H1']\n",
      "[Adamian, 'H1']\n",
      "[Adamian, 'H1']\n",
      "[Adamian, 'H1']\n",
      "[Armenian, 'H1']\n",
      "[Armenian, 'H1']\n",
      "[Armenian, 'H1']\n",
      "[Armenian, 'H1']\n",
      "[Armenian, 'H1']\n",
      "[American, 'H1']\n",
      "[Adamian, 'H1']\n",
      "[Sahag, 'H1']\n",
      "[All Armenians, 'H1']\n",
      "[Adamian, 'H1']\n",
      "[British, 'H1']\n",
      "[British, 'H1']\n",
      "[British, 'H1']\n",
      "[British, 'H1']\n",
      "[Chinese, 'H1']\n",
      "[British, 'H1']\n",
      "[Dentist, 'H1']\n",
      "[Israeli, 'H1']\n",
      "[American, 'H1']\n",
      "[D., 'H1']\n",
      "[Genius, 'H1']\n",
      "[Jesuit, 'H1']\n",
      "[Italian, 'H1']\n",
      "[Australian, 'H1']\n",
      "[Brazilian, 'H1']\n",
      "[Nigerian, 'H1']\n",
      "[Arabic, 'H1']\n",
      "[Arabic, 'H1']\n",
      "[North-Eastern, 'H1']\n",
      "[Jordanian, 'H1']\n",
      "[Muslim, 'H1']\n",
      "[Western, 'H1']\n",
      "[Islamic, 'H1']\n",
      "[Jordanian, 'H1']\n",
      "[Jordanian, 'H1']\n",
      "[Muslim, 'H1']\n",
      "[Islamic, 'H1']\n",
      "[Arab, 'H1']\n",
      "[Israelis, 'H1']\n",
      "[Israelis, 'H1']\n",
      "[Israelis, 'H1']\n",
      "[Palestinians, 'H1']\n",
      "[Jordanian, 'H1']\n",
      "[Israeli, 'H1']\n",
      "[Jews, 'H1']\n",
      "[Jordanian, 'H1']\n",
      "[American, 'H1']\n",
      "[Eastern European Jewish, 'H1']\n",
      "[D., 'H1']\n",
      "[D., 'H1']\n",
      "[English, 'H1']\n",
      "[Artist, 'H1']\n",
      "[British, 'H1']\n",
      "[Jewish, 'H1']\n",
      "[Polish, 'H1']\n",
      "[Brazilian, 'H1']\n",
      "[German, 'H1']\n",
      "[American, 'H1']\n",
      "[Israeli, 'H1']\n",
      "[Ukrainian, 'H1']\n",
      "[Jewish, 'H1']\n",
      "[Brazilian, 'H1']\n",
      "[Egyptian, 'H1']\n",
      "[Egyptian, 'H1']\n",
      "[Egyptian, 'H1']\n",
      "[Upper Egyptian, 'H1']\n",
      "[Egyptian, 'H1']\n",
      "[African, 'H1']\n",
      "[Egyptian, 'H1']\n",
      "[Lebanese, 'H1']\n",
      "[Maronite, 'H1']\n",
      "[Maronites, 'H1']\n",
      "[Aradus, 'H1']\n",
      "[Nigerian, 'H1']\n",
      "[Arabic, 'H1']\n",
      "[Iraqi, 'H1']\n",
      "[Arabic, 'H1']\n",
      "[Arabic, 'H1']\n",
      "[مجلة الأقلام‎, 'H1']\n",
      "[mandaean, 'H1']\n",
      "[Iraqi, 'H1']\n",
      "[Finnish, 'H1']\n",
      "[Russian, 'H1']\n",
      "[German, 'H1']\n",
      "[Serbo-Croatian, 'H1']\n",
      "[Iraqi, 'H1']\n",
      "[Indian, 'H1']\n",
      "[Marxist, 'H1']\n",
      "[Iranian, 'H1']\n",
      "[Iranian, 'H1']\n",
      "[Iranian, 'H1']\n",
      "[Iranian, 'H1']\n",
      "[Iranian, 'H1']\n",
      "[Iranian, 'H1']\n",
      "[Russian, 'H1']\n",
      "[Iranian, 'H1']\n",
      "[Accidental, 'H1']\n",
      "[Iranian, 'H1']\n",
      "[American, 'H1']\n",
      "[lesbian, 'H1']\n",
      "[lesbian, 'H1']\n",
      "[lesbian, 'H1']\n",
      "[lesbian, 'H1']\n",
      "[lesbian, 'H1']\n",
      "[American, 'H1']\n",
      "[Quiet American, 'H1']\n",
      "[Madman, 'H1']\n",
      "[Ballantine, 'H1']\n",
      "[Kurdish, 'H1']\n",
      "[Kurdish, 'H1']\n",
      "[Turkish, 'H1']\n",
      "[Turkish, 'H1']\n",
      "[Kurdish, 'H1']\n",
      "[Turkish, 'H1']\n",
      "[Arabic, 'H1']\n",
      "[Sudanese, 'H1']\n",
      "[Sudanese, 'H1']\n",
      "[Sudanese, 'H1']\n",
      "[anti-Mahdist, 'H1']\n",
      "[non-communist, 'H1']\n",
      "[Sudanese, 'H1']\n",
      "[Ghanaian, 'H1']\n",
      "[Ghanaian, 'H1']\n",
      "[American, 'H1']\n",
      "[American, 'H1']\n",
      "[Aantjes, 'H1']\n",
      "[Dutch, 'H1']\n",
      "[Dutch, 'H1']\n",
      "[Dutch, 'H1']\n",
      "[Nazis, 'H1']\n",
      "[Nazi, 'H1']\n",
      "[Dutch, 'H1']\n",
      "[Dutch, 'H1']\n",
      "[Pakistani, 'H1']\n",
      "[Pakistani, 'H1']\n",
      "[Canadian, 'H1']\n"
     ]
    }
   ],
   "source": [
    "final_output=[]\n",
    "dictText={}\n",
    "dictText['text']=[]\n",
    "dictText['H0']=[]\n",
    "dictText['H1']=[]\n",
    "for index, row in data.iterrows():\n",
    "    sent_mymodel = nlp2(row['Sentence'])\n",
    "    sent_inbuilt=nlp(row['Sentence'])\n",
    "    entities=[]\n",
    "    for ent in sent_inbuilt.ents:\n",
    "        if(ent.label_=='PERSON'):\n",
    "            #print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "            entities.append([ent,'H0'])\n",
    "        elif( ent.label_=='NORP'):\n",
    "            entities.append([ent,'H1'])\n",
    "            #print([ent,'H1'])\n",
    "    for ent in sent_mymodel.ents:\n",
    "        #print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "        entities.append([ent,ent.label_])\n",
    "    #print(row['Sentence'])\n",
    "    words= row['Sentence'].split(\" \")\n",
    "    for word in words:\n",
    "        dictText['text'].append(word)\n",
    "        entity_name=None\n",
    "        #count=0\n",
    "        for entity in entities:\n",
    "            #print(word.translate(table))\n",
    "            #print(entity[0])\n",
    "            #print(word.translate(table) in str(entity[0]))\n",
    "            #print(\"For word:{0}, check entity:{1}, status:{2}\".format(word, entity[0], word.translate(table) in str(entity[0])))\n",
    "            if(word.translate(table) in str(entity[0])):\n",
    "                entity_name=str(entity[1])\n",
    "                #print(\"Word:{0}, EntityName:{1}\".format(word,entity[1])) \n",
    "                break\n",
    "                \n",
    "        if(entity_name!=None):\n",
    "            if(entity_name=='H0'):\n",
    "                dictText['H0'].append(1)\n",
    "            else:\n",
    "                dictText['H0'].append(0)\n",
    "            dictText['H1'].append(1)\n",
    "        else:\n",
    "            dictText['H0'].append(0)\n",
    "            dictText['H1'].append(0)\n",
    "        \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing json\n",
    "import json\n",
    "with open('Output.json', 'w') as outfile:\n",
    "    json.dump(dictText, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
